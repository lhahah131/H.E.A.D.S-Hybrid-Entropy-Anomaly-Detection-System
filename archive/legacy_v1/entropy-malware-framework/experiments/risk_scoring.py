import pandas as pd
import numpy as np 

from sklearn.preprocessing import StandardScaler, MinMaxScaler
from sklearn.ensemble import IsolationForest

#===============
# LOAD DATA
#===============
df = pd.read_csv("features/master_features.csv")

file_ids = df["file_id"]
X = df.drop(columns=["file_id"])

#====================
# SCALING
#====================
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

iso = IsolationForest(
    n_estimators=100,
    contamination=0.12,
    random_state=42
)

iso.fit(X_scaled)
anomaly_scores = iso.decision_function(X_scaled)

minmax = MinMaxScaler()
scaled_scores = minmax.fit_transform((-anomaly_scores).reshape(-1, 1)).flatten()
ml_scores = scaled_scores * 70

#====================
# RULE-BASED SCORING
#====================

def rule_score(row):
    score = 0
    
    if row["non_printable_ratio"] > 0.05:
        score += 3
    if row["avg_line_length"] > 60:
        score += 3
    if row["global_entropy"] > 5.0:
        score += 4
    if row["block_std_entropy"] > 0.5:
        score += 4
    return score
rule_scores = df.apply(rule_score, axis=1)

#====================
# FINAL RISK SCORES
#====================

final_scores = ml_scores + rule_scores

feature_means = df.mean(numeric_only=True)

#======================
# EXPLAINABILITY LAYER
#======================
def generate_explanation(row, feature_means, feature_stds):
    z_scores = {}
    for feature in feature_means.index:
        if feature in row.index and feature_stds[feature] != 0:
            z  = abs((row[feature] - feature_means[feature]) / feature_stds[feature])
            z_scores[feature] = z   

    # ambil yang 3 terbesar
    top_features = sorted(z_scores.items(), key=lambda x: x[1], reverse=True)[:3]
    reasons = [f"{feat} anomaly (z={round(val,2)})" for feat, val in top_features]
    return ", ".join(reasons)


feature_means = df.mean(numeric_only=True)
feature_stds  = df.std(numeric_only=True)

explanition = df.apply(
    lambda row: generate_explanation(row, feature_means, feature_stds),
    axis=1
)

results = pd.DataFrame({
    "file_id": file_ids,
    "ml_score": ml_scores,
    "rule_score": rule_scores,
    "final_score": final_scores,
    "explanation": explanition,
})


def assign_percentile_risk(df):

    p90 = np.percentile(df["final_score"], 90)
    p70 = np.percentile(df["final_score"], 70)
    p40 = np.percentile(df["final_score"], 40)

    def risk_label(score):
        if score >= p90:
            return "CRITICAL"
        elif score >= p70:
            return "HIGH"
        elif score >= p40:
            return "MEDIUM"
        else:
            return "LOW"

    df["risk_level"] = df["final_score"].apply(risk_label)
    return df

results = assign_percentile_risk(results)
results["final_score"] = results["final_score"].round(1)
results = results.sort_values(by="final_score", ascending=False)

pd.set_option("display.max_colwidth", None)
pd.set_option("display.width", None)
pd.set_option("display.max_rows", None)

print("\n==== Risk Scoring Results ====\n")
print(results[["file_id", "final_score", "risk_level", "explanation"]].to_string(index=False))
